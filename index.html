<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content=" Adaptive high-resolution mapping of air pollution with a novel implicit 3D representation approach">
  <meta name="keywords" content="satellite monitoring, air pollution, high-resolution mapping, deep learning, implicit representation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Adaptive high-resolution mapping of air pollution with a novel implicit 3D representation approach</title>

  <meta name="google-site-verification" content="wqG380jQOyUT5HIVYVvh9FR_EnEqKY2aHflQGJ_slJA" />

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!-- <link rel="icon" href="./static/images/favicon.svg"> -->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>




<section class="hero" style="margin-top: 1rem;">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <!-- <h1 class="title is-1 publication-title">VoCo-LLaMA:<br> Towards <span style="color: rgba(95, 173, 242, 0.988);">V</span>isi<span style="color: rgba(95, 173, 242, 0.988);">o</span>n <span style="color: rgba(95, 173, 242, 0.988);">Co</span>mpression with<br>Large Language Models</h1> -->
          <h2 class="title is-2 publication-title"><span style="color: rgba(250, 66, 66, 0.988);">CVPR</span> 2025</h2>
          <div class="is-size-5 publication-authors">
            <!-- <span class="author-block">
              <a href="https://yxxxb.github.io/">Xubing Ye</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=8rltp9AAAAAJ&hl=zh-CN">Yukang Gan</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://xk-huang.github.io/">Xiaoke Huang</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://geyixiao.com/">Yixiao Ge</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=4oXBp9UAAAAJ&hl=en">Ying Shan</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://andytang15.github.io/">Yansong Tang</a><sup>1</sup>
            </span> -->
            <span class="author-block">
              <a >Ting Zhang</a><sup>1</sup>
            <span class="author-block">
              <a >Bo Zheng</a><sup>1</sup>
            <span class="author-block">
              <a >Ruqi Huang</a><sup>1</sup> 
            </span>
          </div> -->

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Tsinghua University,</span>
            <!-- <span class="author-block"><sup>2</sup>ARC Lab, Tencent PCG,</span>
            <span class="author-block"><sup>3</sup>UC Santa Cruz</span> -->
          </div>

          <!-- <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="https://arxiv.org/abs/2406.12275v2"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
              <!-- <span class="link-block">
                <a href="https://arxiv.org/pdf/2406.12275v2"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/Yxxxb/VoCo-LLaMA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span> -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- <div class="column is-3 has-text-centered">
  <img src="./static/images/interpolate_start.jpg"
       class="interpolation-image"
       alt="Interpolate start reference image."/>
  <p>Start Frame</p>
</div> -->


<section class="section" style="margin-top: -4rem;">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">TL;DR</h2>
        <div class="content has-text-justified">
          1. We propose VoCo-LLaMA, the first approach to compress vision tokens using LLMs. 
          By fully utilizing the LLMs' understanding paradigm of vision tokens, our method can compress hundreds of vision tokens into a single VoCo token, while minimizing visual information loss.
          <br>2. VoCo-LLaMA demonstrates the ability to understand video through continuous training using time-series compressed token sequences of video frames.
          <br>3. VoCo-LLaMA presents a promising way to unlock the full potential of VLMs' contextual window.

        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="./static/voco/fig1.jpg"
       class="interpolation-image"
       alt="Interpolate start reference image."/>
      <h2 class="subtitle has-text-centered">
        <br>
        (a) VLMs are bottlenecked by the limited context window when processing high-resolution images and videos. 
        (b) Previous methods compress vision tokens with external modules with substantial loss.
        (c) Illustration of <b>VoCo-LLaMA</b>, which empowers LLM to compress vision tokens and understand compressed tokens via intrinsic token distillation.
      </h2>
    </div>
  </div>
</section>


<section class="section" style="margin-top: -2rem;">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <!-- <p>
            We present the first method capable of photorealistically reconstructing a non-rigidly
            deforming scene using photos/videos captured casually from mobile phones.
          </p>
          Vision-Language Models (VLMs) have achieved remarkable success in various multi-modal tasks, but they are often bottlenecked by the limited context window and high computational cost of processing high-resolution image inputs and videos. 
          Vision compression can alleviate this problem by reducing the vision token count.
          Previous approaches compress vision tokens with external modules and force LLMs to understand the compressed ones, leading to visual information loss.
          However, the LLMs' understanding paradigm of vision tokens is not fully utilised in the compression learning process. -->
          We propose VoCo-LLaMA, the first approach to compress vision tokens using LLMs. 
          We strive to fully utilize the LLMs' understanding paradigm of vision tokens during the compression learning process.
          By introducing <b>V</b>isi<b>o</b>n <b>Co</b>mpression tokens during the vision instruction tuning phase and leveraging attention distillation, our method distill how LLMs comprehend vision tokens into their processing of VoCo tokens.
          VoCo-LLaMA facilitates effective vision compression and improves the computational efficiency during the inference stage.
          Specifically, our method achieves minimal performance loss with a compression ratio of <b>576 x</b>, resulting in up to 94.8% fewer FLOPs and 69.6% acceleration in inference time. 
          Furthermore, through continuous training using time-series compressed token sequences of video frames, VoCo-LLaMA demonstrates the ability to understand temporal correlations, outperforming previous methods on popular video question-answering benchmarks.
          Our approach presents a promising way to unlock the full potential of VLMs' contextual window, enabling more scalable multi-modal applications.

        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    
  </div>
</section>


<section class="section" style="margin-top: -2rem;">
  <div class="container is-max-desktop">


    <!-- Animation. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Vision Compression Design</h2>
        <!-- <div class="content has-text-justified">
          <p>
            Using <span class="dnerf">Nerfies</span>, you can re-render a video from a novel
            viewpoint such as a stabilized camera by playing back the training deformations.
          </p>
        </div> -->
        <div class="content has-text-centered">
          <img src="./static/voco/fig2.jpg"
          class="interpolation-image"
          alt="Interpolate start reference image."/>
        </div>
        <h2 class="subtitle has-text-centered">
          Illustration of the VoCo-LLaMA framework. Based on standard VLMs (a), VoCo-LLaMA (b) first <b>isolate</b> visual and text tokens by injecting VoCo tokens, 
          and then establishes a <b>dedicated interaction pathway</b> between the two modalities via VoCo tokens, 
          enabling the distillation and compression of vision tokens into the transformer activations upon the compact VoCo tokens.<br>
          The compression process of VoCo-LLaMA can be elegantly implemented by strategically modifying the attention mask and learned through standard visual instruction tuning.
        </h2>
        <!--/ Re-rendering. -->

      </div>
    </div>
    <!--/ Animation. -->

  </div>
</section>

<section class="section" style="margin-top: -2rem;">
  <div class="container is-max-desktop">


    <!-- Animation. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Vision Compression Results</h2>
        <!-- <div class="content has-text-justified">
          <p>
            Using <span class="dnerf">Nerfies</span>, you can re-render a video from a novel
            viewpoint such as a stabilized camera by playing back the training deformations.
          </p>
        </div> -->
        <div class="content has-text-centered">
          <img src="./static/voco/exp1.jpg"
          class="interpolation-image"
          alt="Interpolate start reference image."
          style="width: 80%; height: auto;"/> 
        </div>
        <div class="content has-text-centered" style="margin-top: -1rem;">
          <img src="./static/voco/exp2.jpg"
          class="interpolation-image"
          alt="Interpolate start reference image."
          style="width: 80%; height: auto;"/> 
        </div>
        <!--/ Re-rendering. -->

      </div>
    </div>
    <!--/ Animation. -->

  </div>
</section>

<section class="section" style="margin-top: -2rem;">
  <div class="container is-max-desktop">


    <!-- Animation. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Temporal Modeling</h2>
        <div class="content has-text-justified">
          To further investigate the efficacy of our proposed method in handling video input, we utilize the time-series VoCo token sequence obtained by compressing video frames to further explore the temporal modeling capabilities of VoCo-LLaMA.
          Without any additional designs, VoCo-LLaMA outperforms existing vision compression methods on common video question-answering benchmarks, with each video frame compressed into an equal number of tokens. 
        </div>
        <div class="content has-text-centered">
          <img src="./static/voco/fig3.jpg"
          class="interpolation-image"
          alt="Interpolate start reference image."
          style="width: 45%; height: auto;"/> 
        </div>
        <div class="content has-text-centered" style="margin-top: -1rem;">
          <img src="./static/voco/exp3.jpg"
          class="interpolation-image"
          alt="Interpolate start reference image."
          style="width: 80%; height: auto;"/> 
        </div>
        <!--/ Re-rendering. -->

      </div>
    </div>
    <!--/ Animation. -->

  </div>
</section>


<section class="section" style="margin-top: -2rem;">
  <div class="container is-max-desktop">


    <!-- Animation. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Visualization</h2>
        <!-- <div class="content has-text-justified">
          <p>
            Using <span class="dnerf">Nerfies</span>, you can re-render a video from a novel
            viewpoint such as a stabilized camera by playing back the training deformations.
          </p>
        </div> -->
        <div class="content has-text-centered">
          <img src="./static/voco/fig5.jpg"
          class="interpolation-image"
          alt="Interpolate start reference image."
          style="width: 90%; height: auto;"/> 
        </div>

      </div>
    </div>
    <!--/ Animation. -->

  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{ye2024voco,
    author    = {Ye, Xubing and Gan, Yukang and Huang, Xiaoke and Ge, Yixiao and Shan, Ying and Tang, Yansong},
    title     = {{VoCo-LLaMA: Towards Vision Compression with Large Language Models}},
    journal   = {arXiv preprint arXiv:2406.12275},
    year      = {2024},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p> 
            Contact: 
            <a href="mailto:yxb23@mails.tsinghua.edu.cn">yxb23@mails.tsinghua.edu.cn</a>, 
            <a href="mailto:yxb_tongji@163.com">yxb_tongji@163.com</a>.<br>
            This website is adapted from <a
            href="https://github.com/nerfies/nerfies.github.io">Nerfies</a> under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
